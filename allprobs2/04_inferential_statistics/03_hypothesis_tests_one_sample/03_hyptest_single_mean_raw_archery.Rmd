```{r data generation, echo = FALSE, results = "hide"}
knitr::opts_chunk$set(echo = FALSE)
options(scipen=100)

### Target is 600 mm across
while(T){
  sigma = round(2^runif(1,5,7),1)
  n = sample(c(12*1:5,10*2:8),1)
  alpha = 0.05
  SE = sigma/sqrt(n)
  ME = qnorm(1-alpha/2)*SE
  mux = sample(c(-ME,-ME/2,ME/2,ME),1)
  x = round(rnorm(n,mux,sigma))
  y = round(rnorm(n,0,sigma))
  t = abs(mean(x))/(sd(x)/sqrt(n))
  cumulative = pt(t,n-1)
  pval = 2*(1-cumulative)
  if(pval<0.2 & pval>0.001 & abs(pval-0.05)>0.01){break}
}

x2=x

if(pval<0.05){
  ans = "10"
} else {
  ans = "01"
}

default_source_hook <- knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(xx, options) {
  xx <- gsub('x2',paste(x,collapse=","),xx)
  default_source_hook(xx, options)
})
```

Question
========

Archie worries the horizontal aspect of her [sight](https://en.wikipedia.org/wiki/Sight_(device)){target="blank"} may be off. She decides to run a [one-sample t test](https://en.wikipedia.org/wiki/Student's_t-test#Uses){target="blank"} on the $x$ values of her next `r n` arrows. She will use a [significance level](https://en.wikipedia.org/wiki/Statistical_significance){target="blank"} of $\alpha=0.05$. If the result has statistical significance, Archie will adjust her sight.

### Result

```{r bullseye,fig.width=5,fig.height=5}
makebullseye = function(scale=1,axes=T){
  x = cos(2*seq(0,pi,length.out=1000)*pi)*scale
  y = sin(2*seq(0,pi,length.out=1000)*pi)*scale
  plot(x,y,type="l",lwd=3,xlim=c(-scale,scale)*1.1,ylim=c(-scale,scale)*1.1,ann=F,axes=axes)
  lines(x=c(-1,1,1,-1,-1)*scale*1.1,y=c(1,1,-1,-1,1)*scale*1.1,lwd=4)
  colors = c("yellow","yellow","red","red","blue","blue","grey30","grey30","white","white")
  colors2 = lapply(colors, col2rgb)
  linecolors = c(rep("black",6),"white",rep("black",3))
  textcolors = c(rep("black",6),rep("white",2),rep("black",2))
  for(i in 10:1){
    x2 = x*i/10
    y2 = y*i/10
    polygon(x2[1:(length(x2)-1)],y2[1:(length(x2)-1)],col=colors[i],border=F)
    lines(x2,y2,col=linecolors[i])
  }
  points(0,0,pch=20,cex=0.5)
}
par(mar=c(3,3,0,0))
makebullseye(300,T)
mtext("horizontal position, x (mm)",1,line=2)
mtext("vertical position, y (mm)",2,line=2)
abline(v=seq(-300,300,25),lwd=1,lty=2)
abline(h=seq(-300,300,25),lwd=1,lty=2)
points(x,y,pch=20)
```
\

```{r}
write.csv(data.frame(x),"x_arrow.csv",row.names=F)
```

The horizontal positions are shown below and can be [downloaded as a csv](x_arrow.csv){target="blank"}.

```{r comment=NA}
cat(paste(x,collapse=", "))
```

### Analysis

To get the $p$-value:
$$p\text{-value} ~=~ P\left(\big|T\big| > \frac{\big|\bar{x}-\mu_0\big|}{s/\sqrt{n}} \right) $$

In this situation, the null population mean is zero. 
$$\mu_0=0$$
In other words, [the null hypothesis](https://en.wikipedia.org/wiki/Exclusion_of_the_null_hypothesis){target="blank"} claims the sight is correct and the difference between $\bar{x}$ and 0 is just due to chance. The [alternative hypothesis](https://en.wikipedia.org/wiki/Alternative_hypothesis){target="blank"} claims the sight needs to be adjusted: the difference between $\bar{x}$ and 0 is partly because the sight is off.

##ANSWER1##

Archie will adjust her sight if the $p$-value is less than 0.05, because this indicates there is statistical significance. Does Archie adjust her sight?
##ANSWER2##

Answerlist
--------
* Determine the $p$-value.
* Yes.
* No.


Solution
========

```{r}
xbar = mean(x)
s = sd(x)
t = abs(xbar)/(s/sqrt(n))
pval = 2*(1-pt(t,n-1))
alphas = c(0.2,0.1,0.05,0.04,0.02,0.01,0.005,0.004,0.002,0.001)
tt = qt(1-alphas/2,n-1)
ttlo = max(tt[tt<t])
tthi = min(tt[tt>t])
ttlogood = (abs(ttlo-t)<0.03)
tthigood = (abs(tthi-t)<0.03)
alhi = signif(2*(1-pt(tthi,n-1)),3)
allo = signif(2*(1-pt(ttlo,n-1)),3)
thi = round(tthi,2)
tlo = round(ttlo,2)
tol = abs(allo-alhi)/3
```

Determine the sample statistics ($\bar{x}$ and $s$). I would recommend using a spreadsheet or R (or another computer-based method).
$$\bar{x} = `r signif(xbar,4)`$$
$$s = `r signif(s,4)` $$
Evaluate the $t$ statistic.

$$t = \frac{|`r signif(xbar,4)`-0|}{(`r signif(s,4)`)/\sqrt{`r n`}} = `r round(t,2)` $$
Restate the $p$-value.
$$p\text{-value} = P\left(|T|>`r round(t,2)`\right) $$
Remember, the degree of freedom is one less than the sample size.
$$\text{df} = `r n-1` $$

At this point, there are various ways to determine the $p$-value. The least accurate way is to use the $t$-table. Go to the row with $\text{df}=n-1=`r n`-1=`r n-1`$.

```{r,fig.width=8,fig.height=4}
library(latex2exp)
options(scipen=100)
gamma = c(0.8,0.9,0.95,0.96,0.98,0.99,0.995,0.996,0.998,0.999)
left = round(gamma+(1-gamma)/2,4)
right = round(1-left,4)
alpha = round(1-gamma,4)
df = n-1
nrows = length(df)+5
ncols = length(gamma)+1
c1wd = 0.15
cwd = (1-c1wd)/(ncols-1)

par(mar=c(0,0,0,0))
plot(0,0,type="n",axes=F,ann=F,xlim=c(0,1),ylim=c(0,1),xaxs="i",yaxs="i")
for(i in 0:nrows){
  abline(h=i/nrows)
}
abline(v=0)
abline(v=c1wd)
for(i in 1:ncols){
  abline(v=c1wd+cwd*i)
}
lab = c("P(T<t)","P(T>t)","P(|T|<t)","P(|T|>t)")
polygon(c(0,1,1,0),c(1,1,1-4/nrows,1-4/nrows),col=rgb(0.8,0.5,0.5,0.4),lwd=3)
for(i in 1:4){
  ypos = 1-(i-0.5)/nrows
  text(c1wd/2,ypos,lab[i],adj = c(0.5,0.5),family = "mono")
}
for(j in 1:length(gamma)){
    xpos = c1wd+(j-0.5)*cwd
    ypos = 1-(1:4-0.5)/nrows
    text(xpos,ypos,c(left[j],right[j],gamma[j],alpha[j]),family = "mono")
}
polygon(c(0,0,c1wd,c1wd),c(0,1-4/nrows,1-4/nrows,0),lwd=3,col=rgb(0.2,0.4,1,0.2))
polygon(c(c1wd,c1wd,1,1),c(0,1-4/nrows,1-4/nrows,0),lwd=3,col=rgb(0.8,0.8,0.3,0.2))
for(i in seq(5,nrows,2)){
  ylo = 1-i/nrows
  yhi = 1-(i-1)/nrows
  polygon(c(0,1,1,0),c(ylo,ylo,yhi,yhi),col=rgb(0,0,0,0.05))
}
text(c1wd/2,1-(5-0.5)/nrows,"df = ",family = "mono")
text(c1wd+cwd/2,1-(5-0.5)/nrows,"t = ",family = "mono")
for(i in 1:length(df)){
  ypos = 1-(5-0.5)/nrows-i/nrows
  text(c1wd/2,ypos,df[i],family = "mono")
  for(j in 1:length(left)){
    xpos = c1wd+(j-0.5)*cwd
    text(xpos,ypos,sprintf("%.2f",round(qt(left[j],df[i]),2)),family = "mono")
  }
}
```
\


```{r}
if(ttlogood){
  s = paste0("Luckily, in this case, the table has a listed $t$ closely matching ",round(t,2)," in the row with df=",n-1,". This tells us that $$P(|T|>",round(t,2),")\\approx",signif(2*(1-pt(ttlo,n-1)),3),"$$",collapse="")
} else if(tthigood){
  s = paste0("Luckily, in this case, the table has a listed $t$ closely matching ",round(t,2)," in the row with df=",n-1,". This tells us that $$P(|T|>",round(t,2),")\\approx",signif(2*(1-pt(tthi,n-1)),3),"$$",collapse="")
} else {
  alhi = signif(2*(1-pt(tthi,n-1)),3)
  allo = signif(2*(1-pt(ttlo,n-1)),3)
  thi = round(tthi,2)
  tlo = round(ttlo,2)
  tt = round(t,2)
  s = paste0("If we go to the row with df=",n-1,", we can see that our calculated $t$, ",round(t,2),", is between ",tlo," and ",thi,". Thus, we know $P(|T|>",tt,")$ is between ",alhi," and ",allo,'. You could get pretty close by using [linear interpolation](https:////en.wikipedia.org//wiki//Linear_interpolation){target="blank"}. $$P(|T|>',tt,")\\approx \\left(\\frac{",tt,"-",tlo,"}{",thi,"-",tlo,"}\\right)\\left(",alhi,"-",allo,"\\right)+",allo," = ",signif((tt-tlo)/(thi-tlo)*(alhi-allo)+allo,3),"$$")
}

```

`r s`

The more accurate $p$-value is `r pval`. This can be calculated using a computer; for example, you can use [this web app](http://18.191.167.248:3838/Tprob/){target="blank"} (for full accuracy, you'll need the more precise value of $t=`r t`$). You can also use a spreadsheet or R.

### Spreadsheet

```{r}
spacer = rep("",n)
variables = c("alpha","mu0","n","xbar","s","t","P(T<t)","P(|T|>t)","pval","significant?")
formulas = c("0.05","0","=COUNT(A2:A1000)","=AVERAGE(A2:A1000)",
             "=STDEV(A2:A1000)","=ABS(D5-D3)/(D6/SQRT(D4))",
             "=T.DIST(D7,D4-1,1)","=2*(1-D8)","=D9",
             "=D10<D2")
variables=c(variables,rep("",n-length(variables)))
formulas=c(formulas,rep("",n-length(formulas)))
data = data.frame(x,spacer,variables,formulas)
colnames(data)=c("x","","","")
write.csv(data,"t_test_single_sample.csv",row.names=F)
```

```{r fig.width=6,fig.height=4}
draw_ss = function(df,widths=rep(1,length(df[1,])+1)){
  xline = c(0,cumsum(widths)/sum(widths))
  nrow = length(df[,1])
  ncol = length(df[1,])
  xtex = (xline[2:(ncol+2)]+xline[1:(ncol+1)])/2
  par(mar=c(0,0,0,0))
  plot(0,0,type="n",xlim=c(0,1),ylim=c(0,1),axes=F,ann=F)
  hi = 1:((nrow+2)*2+1)
  vi = 1:((ncol+1)*2+1)
  hpos = seq(1,0,length.out=(nrow+2)*2+1)
  vpos = seq(0,1,length.out=(ncol+1)*2+1)
  hlin = hpos[hi%%2==1]
  ytex = hpos[hi%%2==0]
  vlin = vpos[vi%%2==1]
  #xtex = vpos[vi%%2==0]
  abline(h=hlin)
  abline(h=hlin[2],lwd=3)
  abline(v=xline)
  abline(v=xline[2],lwd=3)
  for(i in 2:(ncol+1)){
    text(xtex[i],ytex[1],LETTERS[i-1],col=rgb(0.5,0.5,0.5))
    text(xtex[i],ytex[2],colnames(df)[i-1])
  }    
  for(i in 2:(nrow+2)){
    text(xtex[1],ytex[i],i-1,col=rgb(0.5,0.5,0.5))
  }
  for(i in 1:nrow){
    for(j in 1:ncol){
      text(xtex[j+1],ytex[i+2],df[i,j])
    }
  }
}
draw_ss(data[1:10,],c(1,1,0.3,2,4))
```
\

### R

You can do this problem VERY quickly with R.

```{r echo=TRUE}
x = c(x2)
t.test(x)
```



Meta-information
================
extype: cloze
exsolution: `r pval`|`r ans`
exclozetype: num|schoice
exname: hypothesis test raw
extol: `r tol`|0
exextra[numwidth,logical]: TRUE
exextra[stringwidth,logical]: TRUE
