```{r data generation, echo = FALSE, results = "hide"}
knitr::opts_chunk$set(echo = FALSE)
options(scipen=100)

while(T){
  n = sample(25:200,1)
  ame = 1.96*sqrt(0.5^2/n)
  p = sample(c(0.5-ame,0.5+ame),1)
  x = (runif(n)<p)*1
  phat = mean(x)
  z = abs(phat-0.5)/sqrt(0.5*(1-0.5)/n)
  zlo = floor(z*100)/100
  zhi = ceiling(z*100)/100
  pvallo = 2*(1-round(pnorm(zlo),4))
  pvalhi = 2*(1-round(pnorm(zhi),4))
  pval = 2*(1-pnorm(z))
  pval2 = prop.test(sum(x),n,0.5)$p.value
  pval3 = binom.test(sum(x),n,0.5)$p.value
  if(pval<0.20 & pval>0.005 & abs(pval-0.05)>0.01 & abs(pval2-0.05)>0.005 & abs(pval3-0.05)>0.005
     & abs(pvallo-0.05)>0.005 & abs(pvalhi-0.05)>0.005
     & (pval<0.05)==(pval2<0.05) &  (pval<0.05)==(pval3<0.05)
     & (pval<0.05)==(pvallo<0.05) &  (pval<0.05)==(pvalhi<0.05)){break}
}

write.csv(data.frame(x),"one_prop_test.csv",row.names=F)

x2=x

default_source_hook <- knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(xx, options) {
  xx <- gsub('x2',paste(x,collapse=","),xx)
  default_source_hook(xx, options)
})

ans = ":NUMERICAL:=%s:0.005~%%100%%%s:0.005~%%100%%%s:0.005~%%100%%%s:0.005~%%100%%%s:0.005"
ans = sprintf(ans,pval,pvallo,pvalhi,pval2,pval3)


if(pval<0.05){
  ans2 = "10"
} else {
  ans2 = "01"
}
```

Question
========

Connie suspects a coin may be unfair when spun on its edge on a table. She decides to record some spins, using "0" for tails and "1" for heads. After those spins, she will run a one-proportion hypothesis test using a significance level $\alpha=0.05$.

The data is shown below, and can be [downloaded as a csv](one_prop_test.csv){target="blank"}.

```{r, comment=NA}
cat(paste0(x,collapse=", "))
```

The null hypothesis states the coin is fair and any deviation between the sample proportion and 0.5 is merely due to chance and natural variation. Thus, the null population proportion is $p_0=0.5$.

The alternative hypothesis states the coin is unfair, so the deviation between the sample proportion and 0.5 is at least partly due to the unfairness of the coin.


The $p$-value will indicate the probability that a fair coin produces a sample mean as extreme (or more extreme) in its deviation from 0.5. An approximate formula, using a normal approximation of the proportion sampling and not using a continuity correction, is given:

$$p\text{-value} \approx P\left(|Z| > \frac{|\hat{p}-p_0|}{\sqrt{p_0(1-p_0)/n}} \right) $$
If you want to make a continuity correction, your $p$-value is more accurate (and larger, more conservative).
$$p\text{-value} \approx P\left(|Z| > \frac{(|\hat{p}-p_0|\cdot n-0.5)/n}{\sqrt{p_0(1-p_0)/n}} \right) $$

And, if you want to be exactly correct, you need a computer (or a lot of time) to use the [binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution){target="blank"} formulas. 

$$p\text{-value} = P\left(\left|B(n,p_0)-p_0n\right|\ge \left|n_\text{s}-p_0n\right|\right) $$

For this problem, you can use any of those three strategies.

##ANSWER1##

Connie compares the $p$-value to the significance level, $\alpha=0.05$. If the $p$-value is less than 0.05, Connie concludes the coin is unfair. Otherwise, Connie will conclude the coin MIGHT be fair but future measurements may still show the coin is unfair.

Does Connie conclude the coin is unfair?

##ANSWER2##


Answerlist
--------
* Determine the $p$-value.
* Yes, the sample proportion is significantly far from 0.5, so Connie thinks the coin is unfair.
* No, the sample proportion is NOT significantly far from 0.5, so Connie retains the belief that the coin MIGHT be fair.


Solution
========

First, I apologize for the notation, but for some reason this notation is ubiquitous. We have 4 different "p" variables. 
$$p\text{-value} = \text{probability that null produces result as extreme (or more extreme)}$$
$$P() = \text{general probability notation, with description in parentheses} $$
$$p_0 = \text{the population proportion of the null hypothesis} $$
$$\hat{p} = \text{the sample proportion} $$


You need to determine the sample size ($n$) and sample proportion ($\hat{p}$).

$$n = `r n`$$
$$\hat{p} = \frac{`r sum(x)`}{`r n`} = `r signif(phat,4)` $$

You then calculate a $z$-score. For simplicity, we will not make the continuity correction if we are doing this by hand.

$$z = \frac{|`r signif(phat,4)`-0.5|}{\sqrt{0.5\cdot(1-0.5)/n}} = `r round(z,2)` $$

Restate the $p$-value.
$$p\text{-value} = P(|Z|>`r round(z,2)`)$$

You can use a $z$ table to determine the cumulative probability of $z=`r round(z,2)`$.
$$P(Z<`r round(z,2)`) = `r round(pnorm(round(z,2)),4)`$$

To calculate the $p$-value, you need to remember how to determine a two-tail probability from a cumulative (leftward) probability.
$$p\text{-value} = 2\cdot(1-`r round(pnorm(round(z,2)),4)`) = `r 2*(1-round(pnorm(round(z,2)),4))`$$


### Spreadsheet

```{r spreadsheet}
varis = c("p0","alpha","n","ns","phat","",
          "normal approx, no CC","z","cumulative","pval","",
          "normal approx, CC","z","cumulative","pval","",
          "exact binom","abs success diff","correction for ineq","cumulative","pval")

formulas = c("0.5","0.05","=COUNT(A2:A1000)","=SUM(A2:A1000)","=D5/D4","","",
             "=ABS(D6-D2)/SQRT(D2*(1-D2)/D4)","=NORM.DIST(D9,0,1,1)",
             "=2*(1-D10)","","",
             "=(ABS(D6-D2)*D4-0.5)/D4/SQRT(D2*(1-D2)/D4)",
             "=NORM.DIST(D14,0,1,1)","=2*(1-D15)","","",
             "=ABS(D5-D2*D4)","=D19-1",
             "=BINOM.DIST(D20+D4*D2,D4,D2,1)","=2*(1-D21)")

spacer = rep("",n)
varis = c(varis,rep("",n-length(varis)))
formulas = c(formulas,rep("",n-length(formulas)))
sol = data.frame(x,spacer,varis,formulas)
colnames(sol)=c("x","","","")
write.csv(sol,"one_prop_test_solution.csv",row.names=F)
```

You can use any of the three methods shown in the spreadsheet:

```{r,fig.width=8,fig.height=6}
draw_ss = function(df,widths=rep(1,length(df[1,])+1)){
  xline = c(0,cumsum(widths)/sum(widths))
  nrow = length(df[,1])
  ncol = length(df[1,])
  xtex = (xline[2:(ncol+2)]+xline[1:(ncol+1)])/2
  par(mar=c(0,0,0,0))
  plot(0,0,type="n",xlim=c(0,1),ylim=c(0,1),axes=F,ann=F)
  hi = 1:((nrow+2)*2+1)
  vi = 1:((ncol+1)*2+1)
  hpos = seq(1,0,length.out=(nrow+2)*2+1)
  vpos = seq(0,1,length.out=(ncol+1)*2+1)
  hlin = hpos[hi%%2==1]
  ytex = hpos[hi%%2==0]
  vlin = vpos[vi%%2==1]
  #xtex = vpos[vi%%2==0]
  abline(h=hlin)
  abline(h=hlin[2],lwd=3)
  abline(v=xline)
  abline(v=xline[2],lwd=3)
  for(i in 2:(ncol+1)){
    text(xtex[i],ytex[1],LETTERS[i-1],col=rgb(0.5,0.5,0.5))
    text(xtex[i],ytex[2],colnames(df)[i-1])
  }    
  for(i in 2:(nrow+2)){
    text(xtex[1],ytex[i],i-1,col=rgb(0.5,0.5,0.5))
  }
  for(i in 1:nrow){
    for(j in 1:ncol){
      text(xtex[j+1],ytex[i+2],df[i,j])
    }
  }
}
draw_ss(sol[1:23,],c(0.8,0.8,0.3,2,4))
```
\

You can download a [solution spreadsheet](one_prop_test_solution.csv){target="blank"}.

### R

All three methods can be done easily in R:

```{r echo=T}
x = c(x2)
prop.test(sum(x),length(x),0.5,correct=F)
prop.test(sum(x),length(x),0.5)
binom.test(sum(x),length(x),0.5)
```


Meta-information
================
extype: cloze
exsolution: `r ans`|`r ans2`
exclozetype: verbatim|schoice
exname: hypothesis test 1-prop raw
extol: 0
exextra[numwidth,logical]: TRUE
exextra[stringwidth,logical]: TRUE
